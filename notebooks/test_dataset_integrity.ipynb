{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/10tb/home/fishman/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from streaming.base.format import reader_from_json\n",
    "import numpy as np\n",
    "from streaming.base.spanner import Spanner\n",
    "\n",
    "class NoStreamingDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tA dataset class that can read data with raw mds-format (mosaic streaming-format without compression)\n",
    "\tfrom local. In comparison with `StreamingTextDataset` that also can read data with mds-format from local,\n",
    "\tthis class is slimmer, more efficient, and does not contain redundant code required for streaming.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tlocal: str,\n",
    "\t\tsplit: str,\n",
    "\t\tmax_seq_len: int,\n",
    "\t\ttokenizer = None,\n",
    "\t\tpad_sequences: bool = True,\n",
    "\t) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tif split is not None:\n",
    "\t\t\tsplit_path = os.path.join(local, split)\n",
    "\t\telse:\n",
    "\t\t\tsplit_path = local\n",
    "\t\tindex_file_path = os.path.join(split_path, \"index.json\")\n",
    "\t\tobj = json.load(open(index_file_path))\n",
    "\t\tself.shards = []\n",
    "\t\tfor info in obj[\"shards\"]:\n",
    "\t\t\tshard = reader_from_json(local, split, info)\n",
    "\t\t\traw_filename = os.path.join(shard.dirname, shard.split, shard.raw_data.basename)\n",
    "\t\t\tassert os.path.isfile(raw_filename), f\"Raw file {raw_filename} does not exist\"\n",
    "\t\t\tshard.validate(True)\n",
    "\t\t\tself.shards.append(shard)\n",
    "\t\tsamples_per_shard = np.array([shard.samples for shard in self.shards], np.int64)\n",
    "\t\tself.len = samples_per_shard.sum()\n",
    "\t\tself.spanner = Spanner(samples_per_shard)\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.pad_sequences = pad_sequences\n",
    "\n",
    "\tdef _tokenize(self, text_sample):\n",
    "\t\tassert self.tokenizer is not None, \"Tokenizer required if data is not pretokenized\"\n",
    "\t\t# if self.tokenizer._pad_token is None:\n",
    "\t\t#     # Some tokenizers (e.g. GPT2 tokenizer) have no padding token which causes bugs\n",
    "\t\t#     raise RuntimeError(\"If tokenizing on-the-fly, tokenizer must have a pad_token_id\")\n",
    "\n",
    "\t\treturn self.tokenizer(\n",
    "\t\t\ttext_sample[\"text\"],\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tpadding=\"max_length\" if self.pad_sequences else False,\n",
    "\t\t\tmax_length=self.max_seq_len,\n",
    "\t\t)\n",
    "\n",
    "\tdef __getitem__(self, index: int):\n",
    "\t\tshard_id, shard_sample_id = self.spanner[index]\n",
    "\t\tif index == 303114:\n",
    "\t\t\tprint (shard_id, shard_sample_id)\n",
    "\t\tshard = self.shards[shard_id]\n",
    "\t\tsample = shard[shard_sample_id]\n",
    "\t\tif \"input_ids\" in sample:\n",
    "\t\t\tfor k in list(sample.keys()):\n",
    "\t\t\t\tif isinstance(sample[k], np.ndarray):\n",
    "\t\t\t\t\tsample[k] = sample[k][: self.max_seq_len]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdel sample[k]\n",
    "\t\t\tif \"attention_mask\" not in sample:\n",
    "\t\t\t\tsample[\"attention_mask\"] = np.ones_like(sample[\"input_ids\"])\n",
    "\t\t\treturn sample\n",
    "\t\telif \"text\" in sample:\n",
    "\t\t\ts = self._tokenize(sample)\n",
    "\t\telse:\n",
    "\t\t\tRuntimeError(\"Data sample must contain a field with `input_ids` or `text`\")\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35758782\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_dir = '../data/promoters_mds_full/'\n",
    "\n",
    "dataset = NoStreamingDataset(data_dir, \"train\", 10, tokenizer = lambda x, *args, **kwargs: x)\n",
    "print (len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 143\n"
     ]
    }
   ],
   "source": [
    "dataset.__getitem__(303114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4b812714567a4eeec2e24819f0b13ba  /mnt/nfs_dna/minja/DNALM/promoter_pretrain/train/shard.00163.mds\n"
     ]
    }
   ],
   "source": [
    "!md5sum /mnt/nfs_dna/minja/DNALM/promoter_pretrain/train/shard.00163.mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset integrity check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading samples:   1%|          | 303305/35758782 [05:03<9:40:15, 1018.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading samples:   1%|          | 352232/35758782 [05:51<9:49:23, 1001.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)), desc=\u001b[33m\"\u001b[39m\u001b[33mReading samples\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m \t\u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \t\tsample = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \t\u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      7\u001b[39m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCorrupt sample at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mNoStreamingDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     58\u001b[39m \t\u001b[38;5;28mprint\u001b[39m (shard_id, shard_sample_id)\n\u001b[32m     59\u001b[39m shard = \u001b[38;5;28mself\u001b[39m.shards[shard_id]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m sample = \u001b[43mshard\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshard_sample_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sample:\n\u001b[32m     62\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sample.keys()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bert24/lib/python3.11/site-packages/streaming/base/array.py:90\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, at)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m -\u001b[38;5;28mself\u001b[39m.size <= at < \u001b[32m0\u001b[39m:\n\u001b[32m     89\u001b[39m         at += \u001b[38;5;28mself\u001b[39m.size\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(at, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m     92\u001b[39m     items = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bert24/lib/python3.11/site-packages/streaming/base/format/base/reader.py:319\u001b[39m, in \u001b[36mReader.get_item\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the sample at the index.\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m        Dict[str, Any]: Sample dict.\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decode_sample(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bert24/lib/python3.11/site-packages/streaming/base/format/mds/reader.py:138\u001b[39m, in \u001b[36mMDSReader.get_sample_data\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    136\u001b[39m filename = os.path.join(\u001b[38;5;28mself\u001b[39m.dirname, \u001b[38;5;28mself\u001b[39m.split, \u001b[38;5;28mself\u001b[39m.raw_data.basename)\n\u001b[32m    137\u001b[39m offset = (\u001b[32m1\u001b[39m + idx) * \u001b[32m4\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    139\u001b[39m     fp.seek(offset)\n\u001b[32m    140\u001b[39m     pair = fp.read(\u001b[32m8\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting dataset integrity check...\")\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"Reading samples\"):\n",
    "\ttry:\n",
    "\t\tsample = dataset.__getitem__(i)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"\\nCorrupt sample at index {i}: {e}\\nSample: {sample}\")\n",
    "\t\tbreak\n",
    "\n",
    "print(\"Check finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "original = pd.read_csv('../data/promoters_mds_full/train.md5', sep=' ', header=None, names=['md5','NA', 'fname']).drop(columns=['NA'])\n",
    "H200 = pd.read_csv('../data/promoters_mds_full/train_H200.md5', sep=' ', header=None, names=['md5H200','NA', 'fname']).drop(columns=['NA'])\n",
    "\n",
    "assert len(original) == len(H200)\n",
    "data = pd.merge(original, H200, on='fname', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md5</th>\n",
       "      <th>fname</th>\n",
       "      <th>md5H200</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>4174201032735aaf4615e5f9b8bf54ba</td>\n",
       "      <td>train/shard.02761.mds</td>\n",
       "      <td>2f5b413880ab4568b17061f02bf456b2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>3de611cf19b5e10286f8a44d17124437</td>\n",
       "      <td>train/shard.02788.mds</td>\n",
       "      <td>b1d85d48c8a08ae5901e27db59b3b8a8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>f5de18521c9196d0e6b906d8f5f866cf</td>\n",
       "      <td>train/shard.04098.mds</td>\n",
       "      <td>36f96ec443afc43798085beb7fd7362c</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>dc1def6d9dd37fba48690eda455408e8</td>\n",
       "      <td>train/shard.04691.mds</td>\n",
       "      <td>2ff513289ba58a88b6d10ecc392c4eaf</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>cdc083ec6b2c4576107d9fb3b0f5385f</td>\n",
       "      <td>train/shard.06077.mds</td>\n",
       "      <td>6e5f93659864df7a033d7f50eddaf237</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>40247af76e8a6d3b7bbf5f0cd3439c78</td>\n",
       "      <td>train/shard.06488.mds</td>\n",
       "      <td>53d66485f7709062afc6a074830084ca</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12737</th>\n",
       "      <td>d36ea01a4a32043435c92f0c5819f484</td>\n",
       "      <td>train/shard.12736.mds</td>\n",
       "      <td>7a5fac7a7e8183b43ce371286fc890d7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "      <td>e7527ccac80772c8646af3c4e05d6681</td>\n",
       "      <td>train/shard.15141.mds</td>\n",
       "      <td>018b8eb6400de307453839437a7b5a42</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    md5                  fname  \\\n",
       "2762   4174201032735aaf4615e5f9b8bf54ba  train/shard.02761.mds   \n",
       "2789   3de611cf19b5e10286f8a44d17124437  train/shard.02788.mds   \n",
       "4099   f5de18521c9196d0e6b906d8f5f866cf  train/shard.04098.mds   \n",
       "4692   dc1def6d9dd37fba48690eda455408e8  train/shard.04691.mds   \n",
       "6078   cdc083ec6b2c4576107d9fb3b0f5385f  train/shard.06077.mds   \n",
       "6489   40247af76e8a6d3b7bbf5f0cd3439c78  train/shard.06488.mds   \n",
       "12737  d36ea01a4a32043435c92f0c5819f484  train/shard.12736.mds   \n",
       "15142  e7527ccac80772c8646af3c4e05d6681  train/shard.15141.mds   \n",
       "\n",
       "                                md5H200  diff  \n",
       "2762   2f5b413880ab4568b17061f02bf456b2  True  \n",
       "2789   b1d85d48c8a08ae5901e27db59b3b8a8  True  \n",
       "4099   36f96ec443afc43798085beb7fd7362c  True  \n",
       "4692   2ff513289ba58a88b6d10ecc392c4eaf  True  \n",
       "6078   6e5f93659864df7a033d7f50eddaf237  True  \n",
       "6489   53d66485f7709062afc6a074830084ca  True  \n",
       "12737  7a5fac7a7e8183b43ce371286fc890d7  True  \n",
       "15142  018b8eb6400de307453839437a7b5a42  True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diff'] = data['md5'] != data['md5H200']\n",
    "data[data['diff']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files...\n",
      "\n",
      "Downloading shard.02761.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.02761.mds to tmp/shard.02761.mds\n",
      "MD5 check for shard.02761.mds:\n",
      "Expected:   4174201032735aaf4615e5f9b8bf54ba\n",
      "Downloaded: 4174201032735aaf4615e5f9b8bf54ba\n",
      "Match: True\n",
      "\n",
      "Downloading shard.02788.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.02788.mds to tmp/shard.02788.mds\n",
      "MD5 check for shard.02788.mds:\n",
      "Expected:   3de611cf19b5e10286f8a44d17124437\n",
      "Downloaded: 3de611cf19b5e10286f8a44d17124437\n",
      "Match: True\n",
      "\n",
      "Downloading shard.04098.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.04098.mds to tmp/shard.04098.mds\n",
      "MD5 check for shard.04098.mds:\n",
      "Expected:   f5de18521c9196d0e6b906d8f5f866cf\n",
      "Downloaded: f5de18521c9196d0e6b906d8f5f866cf\n",
      "Match: True\n",
      "\n",
      "Downloading shard.04691.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.04691.mds to tmp/shard.04691.mds\n",
      "MD5 check for shard.04691.mds:\n",
      "Expected:   dc1def6d9dd37fba48690eda455408e8\n",
      "Downloaded: dc1def6d9dd37fba48690eda455408e8\n",
      "Match: True\n",
      "\n",
      "Downloading shard.06077.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.06077.mds to tmp/shard.06077.mds\n",
      "MD5 check for shard.06077.mds:\n",
      "Expected:   cdc083ec6b2c4576107d9fb3b0f5385f\n",
      "Downloaded: cdc083ec6b2c4576107d9fb3b0f5385f\n",
      "Match: True\n",
      "\n",
      "Downloading shard.06488.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.06488.mds to tmp/shard.06488.mds\n",
      "MD5 check for shard.06488.mds:\n",
      "Expected:   40247af76e8a6d3b7bbf5f0cd3439c78\n",
      "Downloaded: 40247af76e8a6d3b7bbf5f0cd3439c78\n",
      "Match: True\n",
      "\n",
      "Downloading shard.12736.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.12736.mds to tmp/shard.12736.mds\n",
      "MD5 check for shard.12736.mds:\n",
      "Expected:   d36ea01a4a32043435c92f0c5819f484\n",
      "Downloaded: d36ea01a4a32043435c92f0c5819f484\n",
      "Match: True\n",
      "\n",
      "Downloading shard.15141.mds...\n",
      "download: s3://genalm/data/pretraining/promoters/train/shard.15141.mds to tmp/shard.15141.mds\n",
      "MD5 check for shard.15141.mds:\n",
      "Expected:   e7527ccac80772c8646af3c4e05d6681\n",
      "Downloaded: e7527ccac80772c8646af3c4e05d6681\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create tmp directory\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "\n",
    "# Get list of files that need to be redownloaded (where diff is True)\n",
    "files_to_check = data[data['diff']]['fname'].tolist()\n",
    "\n",
    "print(\"Checking files...\")\n",
    "for fname in files_to_check:\n",
    "    shard_name = fname.split('/')[-1]\n",
    "    s3_path = f\"s3://genalm/data/pretraining/promoters/train/{shard_name}\"\n",
    "    local_path = f\"tmp/{shard_name}\"\n",
    "    \n",
    "    # Download file\n",
    "    print(f\"\\nDownloading {shard_name}...\")\n",
    "    subprocess.run([\n",
    "        \"aws\", \"s3\", \"cp\", \n",
    "        s3_path, local_path,\n",
    "        \"--endpoint-url\", \"https://s3.cloud.ru\",\n",
    "        \"--profile\", \"airi\"\n",
    "    ])\n",
    "    \n",
    "    # Check MD5\n",
    "    md5_proc = subprocess.run([\"md5sum\", local_path], capture_output=True, text=True)\n",
    "    downloaded_md5 = md5_proc.stdout.split()[0]\n",
    "    expected_md5 = data[data['fname'] == fname]['md5'].iloc[0]\n",
    "    \n",
    "    print(f\"MD5 check for {shard_name}:\")\n",
    "    print(f\"Expected:   {expected_md5}\")\n",
    "    print(f\"Downloaded: {downloaded_md5}\")\n",
    "    print(f\"Match: {downloaded_md5 == expected_md5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train/shard.02761.mds train/shard.02788.mds train/shard.04098.mds train/shard.04691.mds train/shard.06077.mds train/shard.06488.mds train/shard.12736.mds train/shard.15141.mds'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(files_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for f in train/shard.02761.mds train/shard.02788.mds train/shard.04098.mds train/shard.04691.mds train/shard.06077.mds train/shard.06488.mds train/shard.12736.mds train/shard.15141.mds; do shard_name=$(basename $f); echo \"Downloading $shard_name...\"; aws s3 cp \"s3://genalm/data/pretraining/promoters/$shard_name\" \"./$f\" --endpoint-url \"https://s3.cloud.ru\" --profile \"airi\"; done\n"
     ]
    }
   ],
   "source": [
    "files = \" \".join(files_to_check)\n",
    "print(f'for f in {files}; do shard_name=$(basename $f); echo \"Downloading $shard_name...\"; aws s3 cp \"s3://genalm/data/pretraining/promoters/$shard_name\" \"./$f\" --endpoint-url \"https://s3.cloud.ru\" --profile \"airi\"; done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert24)",
   "language": "python",
   "name": "bert24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
